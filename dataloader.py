
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import numpy as np

# generated by deepseek

class CocoCaptionVLMDataset(Dataset):
    """Dataset for Vision-Language Model training with COCO Captions"""
    
    def __init__(self, dataset_split, transform=None, max_length=77, 
                 tokenizer=None, is_train=True, return_image_id=False):
        """
        Args:
            dataset_split: The dataset split from load_dataset (e.g., coco_dataset["train"])
            transform: Image transformations
            max_length: Maximum text sequence length
            tokenizer: Tokenizer for text processing (if None, uses simple tokenization)
            is_train: Whether this is training mode
            return_image_id: Whether to return image_id
        """
        self.dataset = dataset_split
        self.transform = transform
        self.max_length = max_length
        self.tokenizer = tokenizer
        self.is_train = is_train
        self.return_image_id = return_image_id
        
        # Default image transform if none provided
        if self.transform is None:
            self.transform = self._default_transform()
    
    def _default_transform(self):
        """Default image transformations"""
        if self.is_train:
            return transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.RandomHorizontalFlip(),
                transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
        else:
            return transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
    
    def _simple_tokenize(self, text):
        """Simple tokenization if no tokenizer is provided"""
        # Convert to lowercase and split
        tokens = text.lower().split()
        # Pad or truncate to max_length
        if len(tokens) > self.max_length:
            tokens = tokens[:self.max_length]
        else:
            tokens = tokens + ['<pad>'] * (self.max_length - len(tokens))
        return tokens
    
    def _simple_tokenize_with_ids(self, text):
        """Simple tokenization with numerical IDs"""
        tokens = self._simple_tokenize(text)
        # Create a simple vocabulary mapping
        unique_tokens = list(set(tokens))
        vocab = {token: idx for idx, token in enumerate(unique_tokens)}
        token_ids = [vocab.get(token, 0) for token in tokens]
        
        # Create attention mask (1 for real tokens, 0 for padding)
        attention_mask = [1 if token != '<pad>' else 0 for token in tokens]
        
        return {
            'input_ids': torch.tensor(token_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.long)
        }
    
    def __len__(self):
        return len(self.dataset)
    
    def __getitem__(self, idx):
        item = self.dataset[idx]
        
        # Load and transform image
        image = item['image']
        
        # Convert to RGB if needed
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Apply transformations
        if self.transform:
            image_tensor = self.transform(image)
        else:
            # Basic conversion if no transform
            image_tensor = transforms.ToTensor()(image)
        
        # Process text
        text = item['text_input']
        
        if self.tokenizer is not None:
            # Use provided tokenizer (e.g., from transformers library)
            text_encoding = self.tokenizer(
                text,
                max_length=self.max_length,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            # Remove batch dimension added by tokenizer
            input_ids = text_encoding['input_ids'].squeeze(0)
            attention_mask = text_encoding['attention_mask'].squeeze(0)
        else:
            # Use simple tokenization
            encoding = self._simple_tokenize_with_ids(text)
            input_ids = encoding['input_ids']
            attention_mask = encoding['attention_mask']
        
        # Prepare return dictionary
        sample = {
            'image': image_tensor,
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'text': text
        }
        
        if self.return_image_id:
            sample['image_id'] = item.get('image_id', idx)
        
        return sample


class CocoCaptionVLMCollator:
    """Collator for VLM training with dynamic padding"""
    
    def __init__(self, tokenizer=None, max_length=77):
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __call__(self, batch):
        # Separate images, texts, and other fields
        images = [item['image'] for item in batch]
        texts = [item['text'] for item in batch]
        
        # Stack images
        images = torch.stack(images, dim=0)
        
        if self.tokenizer is not None:
            # Tokenize with dynamic padding
            text_encoding = self.tokenizer(
                texts,
                max_length=self.max_length,
                padding=True,
                truncation=True,
                return_tensors='pt'
            )
            input_ids = text_encoding['input_ids']
            attention_mask = text_encoding['attention_mask']
        else:
            # Use existing tokenization from dataset
            input_ids = torch.stack([item['input_ids'] for item in batch])
            attention_mask = torch.stack([item['attention_mask'] for item in batch])
        
        # Create batch dictionary
        batch_dict = {
            'images': images,
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'texts': texts
        }
        
        # Add image_id if present
        if 'image_id' in batch[0]:
            batch_dict['image_ids'] = [item['image_id'] for item in batch]
        
        return batch_dict


# Example usage with HuggingFace transformers tokenizer
def create_vlm_dataloaders(coco_dataset, batch_size=32, num_workers=4, 
                           tokenizer=None, max_length=77):
    """Create train, validation, and test dataloaders for VLM training"""
    
    # Create datasets for each split
    train_dataset = CocoCaptionVLMDataset(
        dataset_split=coco_dataset['train'],
        is_train=True,
        tokenizer=tokenizer,
        max_length=max_length
    )
    
    val_dataset = CocoCaptionVLMDataset(
        dataset_split=coco_dataset['val'],
        is_train=False,
        tokenizer=tokenizer,
        max_length=max_length
    )
    
    test_dataset = CocoCaptionVLMDataset(
        dataset_split=coco_dataset['test'],
        is_train=False,
        tokenizer=tokenizer,
        max_length=max_length
    )
    
    # Create collator
    collator = CocoCaptionVLMCollator(
        tokenizer=tokenizer,
        max_length=max_length
    )
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        collate_fn=collator,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=collator,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=collator,
        pin_memory=True
    )
    
    return {
        'train': train_loader,
        # 'val': val_loader,
        # 'test': test_loader
    }


# Advanced version with multi-modal augmentations
class MultiModalCocoDataset(CocoCaptionVLMDataset):
    """Enhanced VLM dataset with additional capabilities"""
    
    def __init__(self, *args, **kwargs):
        self.enable_text_augmentation = kwargs.pop('enable_text_augmentation', False)
        self.multi_caption = kwargs.pop('multi_caption', False)
        super().__init__(*args, **kwargs)
    
    def _augment_text(self, text):
        """Simple text augmentation"""
        if not self.enable_text_augmentation or not self.is_train:
            return [text]
        
        # Example augmentations (expand based on needs)
        words = text.lower().split()
        if len(words) < 3:
            return [text]
        
        # Create variations
        variations = [
            text,  # Original
            ' '.join(words[1:]),  # Remove first word
            ' '.join(words[:-1]),  # Remove last word
            'a picture of ' + text,
            text + ' in the scene'
        ]
        
        return variations[:3]  # Return up to 3 variations
    
    def __getitem__(self, idx):
        item = self.dataset[idx]
        
        # Process image
        image = item['image'].convert('RGB')
        image_tensor = self.transform(image)
        
        # Process text with possible augmentations
        original_text = item['text_input']
        
        if self.multi_caption:
            # For multi-caption training, get multiple captions
            text_variations = self._augment_text(original_text)
        else:
            text_variations = [original_text]
        
        # Tokenize all text variations
        all_input_ids = []
        all_attention_masks = []
        
        for text in text_variations:
            if self.tokenizer is not None:
                encoding = self.tokenizer(
                    text,
                    max_length=self.max_length,
                    padding='max_length',
                    truncation=True,
                    return_tensors='pt'
                )
                input_ids = encoding['input_ids'].squeeze(0)
                attention_mask = encoding['attention_mask'].squeeze(0)
            else:
                encoding = self._simple_tokenize_with_ids(text)
                input_ids = encoding['input_ids']
                attention_mask = encoding['attention_mask']
            
            all_input_ids.append(input_ids)
            all_attention_masks.append(attention_mask)
        
        # Stack if multiple captions, otherwise keep single
        if len(all_input_ids) > 1:
            input_ids = torch.stack(all_input_ids)
            attention_mask = torch.stack(all_attention_masks)
        else:
            input_ids = all_input_ids[0]
            attention_mask = all_attention_masks[0]
        
        sample = {
            'image': image_tensor,
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'text': original_text,
            'text_variations': text_variations if self.multi_caption else [original_text]
        }
        
        if self.return_image_id:
            sample['image_id'] = item.get('image_id', idx)
        
        return sample


# Example of how to use with actual tokenizer
if __name__ == "__main__":
    # Load your dataset
    from lavis.datasets.builders import load_dataset
    coco_dataset = load_dataset("coco_caption")
    
    # Option 1: Without external tokenizer (uses simple tokenization)
    print("Creating dataloaders without external tokenizer...")
    dataloaders = create_vlm_dataloaders(
        coco_dataset,
        batch_size=16,
        num_workers=2,
        tokenizer=None,  # Will use simple tokenization
        max_length=50
    )
    
    # Test the dataloader
    train_loader = dataloaders['train']
    for batch in train_loader:
        print(f"Batch images shape: {batch['images'].shape}")
        print(f"Batch input_ids shape: {batch['input_ids'].shape}")
        print(f"Batch attention_mask shape: {batch['attention_mask'].shape}")
        print(f"Sample text: {batch['texts'][0]}")
        break
    
    # Option 2: With HuggingFace tokenizer (recommended for actual training)
    try:
        from transformers import AutoTokenizer
        
        print("\nCreating dataloaders with BERT tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
        
        dataloaders_with_tokenizer = create_vlm_dataloaders(
            coco_dataset,
            batch_size=16,
            num_workers=2,
            tokenizer=tokenizer,
            max_length=50
        )
        
        # Test with tokenizer
        train_loader = dataloaders_with_tokenizer['train']
        for batch in train_loader:
            print(f"Batch images shape: {batch['images'].shape}")
            print(f"Batch input_ids shape: {batch['input_ids'].shape}")
            print(f"Decoded text: {tokenizer.decode(batch['input_ids'][0])}")
            break
            
    except ImportError:
        print("Transformers library not installed. Install with: pip install transformers")



# from lavis.datasets.builders import load_dataset
# coco_dataset = load_dataset("coco_caption")
# print(coco_dataset.keys())
# # dict_keys(['train', 'val', 'test'])
# print(len(coco_dataset["train"]))
# # 566747
# print(coco_dataset["train"][0])
# # {'image': <PIL.Image.Image image mode=RGB size=640x480>,
# #  'text_input': 'A woman wearing a net on her head cutting a cake. ',
# #  'image_id': 0}


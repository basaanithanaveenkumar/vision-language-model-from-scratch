import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter
from models.vlm import BasicVLM
import torchvision.transforms as transforms
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import io
import cv2
import tempfile
import os
from datetime import datetime
from models.ha_vlm import HaloVLM


# majority of the code is generated by the copilot and deepseek for better logging features into the tensorboard

class BasicVLMTrainer:
    def __init__(self, model, train_loader, val_loader=None, 
                 device='cuda', learning_rate=1e-4, max_epochs=10,
                 log_tensorboard=False, log_dir="./runs"):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.max_epochs = max_epochs
        
        # Setup optimizer
        self.optimizer = optim.AdamW(
            self.model.parameters(), 
            lr=learning_rate,
            weight_decay=0.01
        )
        
        # Setup scheduler
        self.scheduler = CosineAnnealingLR(
            self.optimizer, 
            T_max=max_epochs * len(train_loader)
        )
        
        # Loss function - ignore padding tokens
        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)
        
        # Setup logging
        self.log_tensorboard = log_tensorboard
        self.writer = None
        if log_tensorboard:
            self.writer = SummaryWriter(log_dir=log_dir)
        
        self.best_val_loss = float('inf')
        self.global_step = 0
        self.tokenizer = None  # Will be set externally if needed
        
    def set_tokenizer(self, tokenizer):
        """Set tokenizer for decoding predictions"""
        self.tokenizer = tokenizer
    
    def log_gradients(self, step):
        """Log gradient statistics to tensorboard"""
        if not self.log_tensorboard or self.writer is None:
            return
        
        try:
            for name, param in self.model.named_parameters():
                if param.grad is not None:
                    # Log gradient norm
                    grad_norm = param.grad.data.norm(2)
                    self.writer.add_scalar(f'gradients/{name}/norm', grad_norm, step)
                    
                    # Log gradient statistics
                    self.writer.add_scalar(f'gradients/{name}/mean', param.grad.data.mean(), step)
                    self.writer.add_scalar(f'gradients/{name}/std', param.grad.data.std(), step)
                    self.writer.add_scalar(f'gradients/{name}/max', param.grad.data.max(), step)
                    self.writer.add_scalar(f'gradients/{name}/min', param.grad.data.min(), step)
                    
                    # Log gradient histogram
                    self.writer.add_histogram(f'gradients/{name}', param.grad.data, step)
        except Exception as e:
            print(f"Warning: Could not log gradients: {e}")
    
    def log_weight_stats(self, step):
        """Log weight statistics to tensorboard"""
        if not self.log_tensorboard or self.writer is None:
            return
        
        try:
            for name, param in self.model.named_parameters():
                if param.requires_grad:
                    # Log weight norm
                    weight_norm = param.data.norm(2)
                    self.writer.add_scalar(f'weights/{name}/norm', weight_norm, step)
                    
                    # Log weight statistics
                    self.writer.add_scalar(f'weights/{name}/mean', param.data.mean(), step)
                    self.writer.add_scalar(f'weights/{name}/std', param.data.std(), step)
                    
                    # Log weight histogram
                    self.writer.add_histogram(f'weights/{name}', param.data, step)
        except Exception as e:
            print(f"Warning: Could not log weight stats: {e}")
    
    def check_gradients(self):
        """Check for None gradients and print parameters with missing gradients"""
        print("\n" + "="*80)
        print("GRADIENT CHECK REPORT")
        print("="*80)
        
        none_gradient_params = []
        valid_gradient_params = []
        total_params_with_grad = 0
        total_params_without_grad = 0
        none_gradient_param_count = 0
        
        for name, param in self.model.named_parameters():
            total_params = param.numel()
            
            if param.grad is None:
                none_gradient_params.append({
                    'name': name,
                    'shape': tuple(param.shape),
                    'num_params': total_params,
                    'requires_grad': param.requires_grad,
                    'dtype': str(param.dtype)
                })
                if param.requires_grad:
                    none_gradient_param_count += total_params
                total_params_without_grad += total_params
            else:
                valid_gradient_params.append({
                    'name': name,
                    'shape': tuple(param.shape),
                    'num_params': total_params,
                    'grad_norm': param.grad.data.norm(2).item(),
                    'dtype': str(param.dtype)
                })
                total_params_with_grad += total_params
        
        # Print summary
        print(f"\n✓ Parameters WITH gradients: {len(valid_gradient_params)}")
        print(f"  Total parameter count: {total_params_with_grad:,}")
        
        print(f"\n✗ Parameters WITH NONE gradients: {len(none_gradient_params)}")
        print(f"  Total parameter count: {total_params_without_grad:,}")
        print(f"  Parameters with requires_grad=True but grad=None: {none_gradient_param_count:,}")
        
        # Print details of None gradient parameters
        if none_gradient_params:
            print(f"\n{'─'*80}")
            print("DETAILS OF PARAMETERS WITH NONE GRADIENTS:")
            print(f"{'─'*80}")
            print(f"{'Layer Name':<50} {'Shape':<15} {'Num Params':<12} {'Requires Grad':<15}")
            print(f"{'-'*80}")
            
            for param_info in none_gradient_params:
                requires_grad = "✓ Yes" if param_info['requires_grad'] else "✗ No"
                shape_str = str(param_info['shape'])
                print(f"{param_info['name']:<50} {shape_str:<15} {param_info['num_params']:<12,} {requires_grad:<15}")
        
        # Print details of valid gradient parameters (first 10)
        if valid_gradient_params:
            print(f"\n{'─'*80}")
            print("DETAILS OF PARAMETERS WITH VALID GRADIENTS (first 10):")
            print(f"{'─'*80}")
            print(f"{'Layer Name':<50} {'Shape':<15} {'Grad Norm':<15}")
            print(f"{'-'*80}")
            
            for i, param_info in enumerate(valid_gradient_params[:10]):
                shape_str = str(param_info['shape'])
                print(f"{param_info['name']:<50} {shape_str:<15} {param_info['grad_norm']:<15.6e}")
            
            if len(valid_gradient_params) > 10:
                print(f"... and {len(valid_gradient_params) - 10} more parameters with valid gradients")
        
        # Print overall statistics
        print(f"\n{'─'*80}")
        print("OVERALL STATISTICS:")
        print(f"{'─'*80}")
        total_model_params = sum(p.numel() for p in self.model.parameters())
        print(f"Total model parameters: {total_model_params:,}")
        print(f"Parameters with gradients: {total_params_with_grad:,} ({100*total_params_with_grad/total_model_params:.2f}%)")
        print(f"Parameters without gradients: {total_params_without_grad:,} ({100*total_params_without_grad/total_model_params:.2f}%)")
        
        if none_gradient_param_count > 0:
            print(f"\n⚠️  WARNING: {none_gradient_param_count:,} trainable parameters have None gradients!")
            print(f"   This may indicate frozen layers or computational graph issues.")
        else:
            print(f"\n✓ All trainable parameters have valid gradients!")
        
        print("="*80 + "\n")
        
        return {
            'total_params': total_model_params,
            'params_with_grad': total_params_with_grad,
            'params_without_grad': total_params_without_grad,
            'trainable_without_grad': none_gradient_param_count,
            'none_gradient_params': none_gradient_params,
            'valid_gradient_params': valid_gradient_params
        }
    
    def tensor_to_image(self, img_tensor):
        """Convert normalized image tensor to PIL Image"""
        # Handle batch dimension
        if img_tensor.dim() == 4:
            img_tensor = img_tensor[0]
        
        # Denormalize (ImageNet normalization)
        denormalize = transforms.Compose([
            transforms.Normalize(
                mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
                std=[1/0.229, 1/0.224, 1/0.225]
            )
        ])
        
        img_tensor = denormalize(img_tensor.cpu())
        img_tensor = torch.clamp(img_tensor, 0, 1)
        img_array = img_tensor.permute(1, 2, 0).numpy()
        img_array = (img_array * 255).astype(np.uint8)
        return Image.fromarray(img_array)
    def visualize_predictions(self, images, input_ids, outputs, texts=None, num_samples=2, num_img_tokens=196):
        """
        Visualize predicted vs ground truth tokens with attractive formatting.
        Shows ground truth (GT) in green and predictions in blue with color-coded accuracy.
        
        Args:
            num_img_tokens: Number of image tokens to skip (default 196 for patch-based encoders)
        """
        num_samples = min(num_samples, images.size(0))
        visualizations = []
        
        for idx in range(num_samples):
            img = self.tensor_to_image(images[idx:idx+1])
            img_width, img_height = img.size
            
            # Get model predictions for this sample
            # Skip image tokens - only get text predictions
            logits = outputs[idx, num_img_tokens:, :]  # [text_seq_len, vocab_size]
            predictions = torch.argmax(logits, dim=-1)  # [text_seq_len]
            confidences = torch.softmax(logits, dim=-1).max(dim=-1)[0]  # [text_seq_len]
            
            # Ground truth is the labels (next tokens)
            # Since labels align with text positions, we use input_ids shifted by 1
            ground_truth = input_ids[idx, 1:]  # [seq_len-1] - next tokens
            
            # Trim predictions to match ground truth length
            predictions = predictions[:len(ground_truth)]
            confidences = confidences[:len(ground_truth)]
            
            # Get top-k predictions for alternative options
            top_k = 3
            top_probs, top_indices = torch.topk(torch.softmax(logits[:len(ground_truth)], dim=-1), top_k, dim=-1)
            
            # Create a larger canvas for better visualization
            canvas_height = img_height + 600
            
            # Try to use a better font if available
            try:
                title_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 16)
                text_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 13)
                small_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 11)
            except:
                title_font = ImageFont.load_default()
                text_font = ImageFont.load_default()
                small_font = ImageFont.load_default()
            
            # Calculate text content to determine canvas width
            gt_line = "GT: "
            for token_id in ground_truth[:min(20, len(ground_truth))]:
                token_id_item = token_id.item()
                if self.tokenizer:
                    try:
                        token_text = self.tokenizer.decode([token_id_item])
                        token_text = token_text.replace('[CLS]', '').replace('[SEP]', '').replace('[PAD]', '').strip()
                        if token_text:
                            gt_line += f"[{token_text}] "
                    except:
                        gt_line += f"[ID:{token_id_item}] "
                else:
                    gt_line += f"[{token_id_item}] "
            
            pred_line = "PRED: "
            for i, token_id in enumerate(predictions[:min(20, len(ground_truth))]):
                token_id_item = token_id.item()
                if self.tokenizer:
                    try:
                        token_text = self.tokenizer.decode([token_id_item])
                        token_text = token_text.replace('[CLS]', '').replace('[SEP]', '').replace('[PAD]', '').strip()
                        if token_text:
                            pred_line += f"[{token_text}] "
                    except:
                        pred_line += f"[ID:{token_id_item}] "
                else:
                    pred_line += f"[{token_id_item}] "
            
            # Use a temporary image to measure text
            temp_img = Image.new('RGB', (1, 1))
            temp_draw = ImageDraw.Draw(temp_img)
            
            try:
                gt_width = temp_draw.textbbox((0, 0), gt_line, font=text_font)[2] - temp_draw.textbbox((0, 0), gt_line, font=text_font)[0]
                pred_width = temp_draw.textbbox((0, 0), pred_line, font=text_font)[2] - temp_draw.textbbox((0, 0), pred_line, font=text_font)[0]
            except:
                gt_width = len(gt_line) * 8
                pred_width = len(pred_line) * 8
            
            # Set canvas width
            max_text_width = max(gt_width, pred_width)
            min_canvas_width = img_width
            required_width = max_text_width + 60
            canvas_width = max(min_canvas_width, required_width)
            
            canvas = Image.new('RGB', (canvas_width, canvas_height), color=(15, 15, 15))
            canvas.paste(img, (0, 0))
            
            draw = ImageDraw.Draw(canvas)
            
            y_offset = img_height + 15
            
            # Title section
            draw.text((15, y_offset), f"NEXT TOKEN PREDICTION (Skip first {num_img_tokens} image tokens)", 
                    fill=(100, 200, 255), font=title_font)
            y_offset += 35
            
            # Ground truth sequence
            draw.text((15, y_offset), "Ground Truth Sequence:", fill=(100, 255, 100), font=title_font)
            y_offset += 28
            draw.text((15, y_offset), gt_line, fill=(100, 255, 100), font=text_font)
            y_offset += 28
            
            # Predicted sequence
            draw.text((15, y_offset), "Predicted Sequence:", fill=(100, 150, 255), font=title_font)
            y_offset += 28
            draw.text((15, y_offset), pred_line, fill=(100, 150, 255), font=text_font)
            y_offset += 35
            
            # Detailed position analysis
            draw.text((15, y_offset), "Position-wise Analysis (First 10 text tokens):", 
                    fill=(200, 200, 200), font=title_font)
            y_offset += 28
            
            # Create position analysis table
            for pos in range(min(10, len(predictions))):
                pred_token = predictions[pos].item()
                confidence = confidences[pos].item()
                
                # Determine if prediction is correct
                is_correct = False
                if pos < len(ground_truth):
                    gt_token = ground_truth[pos].item()
                    is_correct = (pred_token == gt_token)
                
                # Color based on correctness and confidence
                if is_correct:
                    status_color = (100, 255, 100)  # Green
                    status_text = "✓"
                elif confidence > 0.6:
                    status_color = (255, 255, 100)  # Yellow
                    status_text = "≈"
                else:
                    status_color = (255, 100, 100)  # Red
                    status_text = "✗"
                
                # Build position line (show actual position in sequence after image tokens)
                actual_pos = num_img_tokens + pos
                pos_line = f"[{actual_pos:3d}→{pos:2d}] {status_text} "
                
                # Ground truth
                if pos < len(ground_truth):
                    gt_token = ground_truth[pos].item()
                    if self.tokenizer:
                        try:
                            gt_text = self.tokenizer.decode([gt_token]).strip()
                            pos_line += f"GT: '{gt_text}' | "
                        except:
                            pos_line += f"GT: ID{gt_token} | "
                    else:
                        pos_line += f"GT: ID{gt_token} | "
                
                # Prediction with confidence
                if self.tokenizer:
                    try:
                        pred_text = self.tokenizer.decode([pred_token]).strip()
                        pos_line += f"PRED: '{pred_text}' ({confidence:.2%})"
                    except:
                        pos_line += f"PRED: ID{pred_token} ({confidence:.2%})"
                else:
                    pos_line += f"PRED: ID{pred_token} ({confidence:.2%})"
                
                draw.text((15, y_offset), pos_line, fill=status_color, font=small_font)
                y_offset += 22
            
            # Add legend at the bottom
            y_offset += 10
            draw.text((15, y_offset), "Legend:", fill=(200, 200, 200), font=title_font)
            y_offset += 22
            draw.text((15, y_offset), "✓ Correct", fill=(100, 255, 100), font=small_font)
            draw.text((250, y_offset), "≈ Confident (~60%+)", fill=(255, 255, 100), font=small_font)
            draw.text((550, y_offset), "✗ Low confidence", fill=(255, 100, 100), font=small_font)
            
            # Calculate and display accuracy
            y_offset += 30
            accuracy = (predictions[:len(ground_truth)] == ground_truth).float().mean().item()
            accuracy_color = (100, 255, 100) if accuracy > 0.7 else (255, 255, 100) if accuracy > 0.4 else (255, 100, 100)
            draw.text((15, y_offset), f"Accuracy (Top-1): {accuracy:.2%} | Valid tokens: {len(ground_truth)}", 
                    fill=accuracy_color, font=title_font)
            
            visualizations.append(canvas)
        
        return visualizations
    def generate_token_prediction_video(self, images, input_ids, outputs, step=0, fps=2, num_samples=1, num_img_tokens=196, num_question_tokens=4):
        """
        Generate video with sharp text and clear spacing between tokens.
        """
        try:
            import imageio
            import cv2
            import numpy as np
            from PIL import Image, ImageDraw, ImageFont
        except ImportError:
            print("Warning: libraries not installed. pip install imageio opencv-python numpy pillow")
            return []
        
        os.makedirs('./videos', exist_ok=True)
        num_samples = min(num_samples, images.size(0))
        saved_paths = []
        # TODO Move this color config into the json loadable
        # Enhanced Color Palette (High contrast for sharpness)
        COLORS  = {
            'overlay_bg': (20, 21, 26, 200),    # Deep Blue-Black (High opacity for text legibility)
            'correct': (50, 255, 126),          # Neon Mint Green (Clear success indicator)
            'incorrect': (255, 71, 87),         # Neon Coral Red (Clear error indicator)
            'gt_text': (24, 220, 255),          # Electric Cyan (Ground Truth)
            'pred_text': (197, 108, 240),       # Bright Lavender (Predictions)
            'question': (255, 242, 0),          # Electric Yellow (Questions)
            'header': (200, 214, 229),          # Cool White-Grey (Headers)
            'vision_info': (255, 159, 243),     # Neon Pink (Metadata)
            'separator': (255, 255, 255, 50)    # Subtle White (Dividers)
        }
        
        # Spacing Configuration
        TOKEN_PADDING = 12  # Space (pixels) between each token
        LINE_HEIGHT = 35    # Vertical space between lines
        
        for sample_idx in range(num_samples):
            try:
                idx = sample_idx
                # Process Image
                img_tensor = images[idx:idx+1]
                base_img = self.tensor_to_image(img_tensor).convert("RGBA")
                img_width, img_height = base_img.size
                
                # Logic to handle model outputs
                logits = outputs[idx]
                predictions = torch.argmax(logits, dim=-1)
                
                # Slicing tokens
                text_predictions = predictions[num_img_tokens:]
                ground_truth = input_ids[idx, 1:] 
                
                # Setup Fonts - Prioritizing Bold for Sharpness
                try:
                    # Linux standard paths
                    font_path = "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf"
                    # Try to load; if fails, fall back to default
                    main_font = ImageFont.truetype(font_path, 20)  # Larger size for sharpness
                    header_font = ImageFont.truetype(font_path, 15)
                    small_font = ImageFont.truetype(font_path, 13)
                except:
                    # Fallback if specific font not found
                    main_font = header_font = small_font = ImageFont.load_default()

                # Video Dimensions
                video_width = max(img_width, 900)  # Slightly wider to accommodate spaced text
                video_height = max(img_height, 700)
                
                if base_img.size != (video_width, video_height):
                    base_img = base_img.resize((video_width, video_height), Image.Resampling.LANCZOS)

                frames = []
                max_tokens = min(30, len(text_predictions))
                
                # Pre-decode GT
                gt_words = []
                for token_id in ground_truth[:max_tokens]:
                    try:
                        # Replace special tokenizer chars with visual space or empty
                        word = self.tokenizer.decode([token_id.item()]).replace("Ġ", "")
                        if word.strip() == "": word = "␣" # Visual placeholder for pure space tokens
                        gt_words.append(word)
                    except:
                        gt_words.append("?")

                # Animation Loop
                for pos in range(max_tokens):
                    # 1. Base Frame
                    frame = base_img.copy()
                    
                    # 2. Overlay
                    overlay = Image.new('RGBA', (video_width, video_height), (0,0,0,0))
                    draw = ImageDraw.Draw(overlay)
                    
                    # Panel Logic
                    panel_h = int(video_height * 0.45)
                    panel_y = video_height - panel_h
                    
                    # Background
                    draw.rectangle([(0, panel_y), (video_width, video_height)], fill=COLORS['overlay_bg'])
                    draw.line([(0, panel_y), (video_width, panel_y)], fill=COLORS['separator'], width=2)
                    
                    margin_x = 30
                    current_y = panel_y + 25
                    
                    # --- HEADER ---
                    draw.text((margin_x, current_y), f"Vision Tokens: {num_img_tokens}", fill=COLORS['vision_info'], font=small_font)
                    
                    # Accuracy
                    if pos >= num_question_tokens:
                        matches = 0
                        count = 0
                        for i in range(num_question_tokens, pos + 1):
                            if i < len(ground_truth) and text_predictions[i] == ground_truth[i]:
                                matches += 1
                            count += 1
                        acc = (matches / max(count, 1)) * 100
                        acc_text = f"Accuracy: {acc:.1f}%"
                        acc_color = COLORS['correct'] if acc > 70 else COLORS['incorrect']
                        
                        bbox = draw.textbbox((0,0), acc_text, font=header_font)
                        draw.text((video_width - margin_x - (bbox[2]-bbox[0]), current_y), acc_text, fill=acc_color, font=header_font)
                    
                    current_y += 30
                    
                    # --- GROUND TRUTH ---
                    draw.text((margin_x, current_y), "GROUND TRUTH:", fill=COLORS['header'], font=header_font)
                    current_y += 25
                    
                    gt_x = margin_x
                    gt_line_y = current_y
                    
                    for i in range(min(pos + 5, len(gt_words))):
                        word = gt_words[i]
                        color = COLORS['question'] if i < num_question_tokens else COLORS['gt_text']
                        
                        draw.text((gt_x, gt_line_y), word, fill=color, font=main_font)
                        
                        # Spacing logic
                        w_bbox = draw.textbbox((0,0), word, font=main_font)
                        word_width = w_bbox[2] - w_bbox[0]
                        gt_x += word_width + TOKEN_PADDING  # Add padding between tokens
                        
                        if gt_x > video_width - margin_x - 50:
                            gt_x = margin_x
                            gt_line_y += LINE_HEIGHT
                            
                    current_y = gt_line_y + 45
                    
                    # --- PREDICTIONS ---
                    draw.text((margin_x, current_y), "MODEL PREDICTION:", fill=COLORS['header'], font=header_font)
                    current_y += 25
                    
                    pred_x = margin_x
                    pred_line_y = current_y
                    
                    for i in range(pos + 1):
                        token_id = text_predictions[i].item()
                        try:
                            word = self.tokenizer.decode([token_id]).replace("Ġ", "")
                            if word.strip() == "": word = "␣"
                        except:
                            word = "?"
                            
                        # Color Logic
                        if i < num_question_tokens:
                            color = COLORS['question']
                        else:
                            is_correct = (i < len(ground_truth) and token_id == ground_truth[i].item())
                            color = COLORS['correct'] if is_correct else COLORS['incorrect']
                        
                        # Draw Text
                        draw.text((pred_x, pred_line_y), word, fill=color, font=main_font)
                        
                        # Active Cursor Underline
                        w_bbox = draw.textbbox((0,0), word, font=main_font)
                        word_width = w_bbox[2] - w_bbox[0]
                        
                        if i == pos:
                            draw.line([(pred_x, pred_line_y + 25), (pred_x + word_width, pred_line_y + 25)], 
                                     fill=COLORS['separator'], width=3)

                        # Advance X with spacing
                        pred_x += word_width + TOKEN_PADDING
                        
                        # Wrap
                        if pred_x > video_width - margin_x - 50:
                            pred_x = margin_x
                            pred_line_y += LINE_HEIGHT

                    # 3. Save Frame
                    final_frame = Image.alpha_composite(frame, overlay)
                    frames.append(np.array(final_frame.convert("RGB")))
                
                # Write Video
                if frames:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    video_path = f"./videos/token_pred_step{step}_sample{sample_idx}_{timestamp}.mp4"
                    imageio.mimwrite(video_path, frames, fps=fps, codec='libx264', quality=9)
                    saved_paths.append(video_path)
                    print(f"✓ Saved video: {video_path}")

            except Exception as e:
                print(f"Error processing sample {sample_idx}: {e}")
                import traceback
                traceback.print_exc()
                continue
                
        return saved_paths
    def train_epoch(self, epoch):
        self.model.train()
        total_loss = 0
        total_tokens = 0
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}")
        for batch_idx, batch in enumerate(pbar):
        # single batch overfitting code
        # single_batch = next(iter(self.train_loader))

        # # Train on this single batch repeatedly
        # pbar = tqdm(range(len(self.train_loader)), desc=f"Epoch {epoch+1}")
        # for batch_idx in pbar:
        #     batch = single_batch
            # Move data to device
            images = batch['images'].to(self.device)
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)  # ✓ ADD THIS
            labels = batch['labels'].to(self.device)  # ✓ USE LABELS FROM DATALOADER
            
            # Validate that labels are present
            if labels is None:
                raise ValueError("Batch['labels'] is None! Dataloader must provide labels.")
            
            
            # Forward pass
            self.optimizer.zero_grad()
            
            # Get model outputs
            #outputs = self.model(images, input_ids)
            outputs = self.model(images, input_ids, attention_mask=attention_mask)
            
            # Prepare targets for loss calculation
            # For next-token prediction:
            # Input sequence to model: [IMG_TOKEN, text_0, text_1, ..., text_{n-1}]  (n+1 tokens total)
            # Model output at pos i:   predicts the token that comes AFTER position i
            # 
            # So: output[0] (at IMG_TOKEN) → should predict text_0
            #     output[1] (at text_0)    → should predict text_1
            #     output[i]                → should predict text_i
            #     output[n]                → should predict text_n (but we don't have this in input_ids)
            #
            # Therefore:
            # - image_targets at pos 0:     -100 (ignore, image token has no language target)
            # - text_targets at pos 1..n:   input_ids[1:n+1] (the next tokens)
            # - pos n+1:                    ignored (no target available)
            
            num_img_tokens = 196
            B = input_ids.shape[0]
            seq_len = input_ids.shape[1]  # Original text sequence length
            
            # Create targets with proper shifting
            image_targets = torch.full((B, num_img_tokens), -100, device=self.device)  # [B, 1]
            text_targets = labels  # [B, seq_len-1] - next tokens for text positions
            
            # Concatenate: [image_targets (ignore) | text_targets (actual predictions)]
            # Final shape: [B, 1 + (seq_len-1)] = [B, seq_len]
            targets = torch.cat([image_targets, text_targets], dim=1)
            # use labels directly without concatenation and shifting as we have already added image token in the beginnning
            #targets = text_targets
            # Model output shape: [B, 1 + seq_len, vocab_size]
            # We only compute loss for first seq_len positions (last position is discarded)
            # outputs_for_loss shape: [B, seq_len, vocab_size]
            # outputs_for_loss = outputs[:, :seq_len, :]
            outputs_for_loss = outputs[:, :targets.shape[1], :]
            assert outputs_for_loss.shape[0] == targets.shape[0], f"Batch size mismatch: {outputs_for_loss.shape[0]} vs {labels.shape[0]}"
            assert outputs_for_loss.shape[1] == targets.shape[1], f"Sequence length mismatch: {outputs_for_loss.shape[1]} vs {labels.shape[1]}"
            # for debugging
            #print(input_ids[0], "input ids")
            #print(targets[0], "tagrgets")
            
            # Calculate loss
            # This computes cross-entropy for:
            # - Position 0 (image): loss ignored due to -100 target
            # - Positions 1..seq_len-1: actual next-token prediction losses
            loss = self.criterion(
            outputs_for_loss.reshape(-1, outputs_for_loss.size(-1)),
            targets.reshape(-1)  # targets are already shifted and padded properly
            )
            
            # for debugging
            # Print output IDs (predictions) after loss calculation
            output_ids = torch.argmax(outputs_for_loss, dim=-1)  # [B, seq_len]
            
            
            #print(output_ids[0], "output ids")

            # Backward pass
            loss.backward()
            
            # Check gradients on first batch of first epoch
            if epoch == 0 and batch_idx == 0:
                print("\nChecking gradients after first backward pass...")
                self.check_gradients()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            
            # Optimizer step
            self.optimizer.step()
            self.scheduler.step()
            
            # Calculate token-level loss for logging
            batch_tokens = (targets != -100).sum().item()
            batch_loss = loss.item() * batch_tokens
            
            total_loss += batch_loss
            total_tokens += batch_tokens
            
            # Update progress bar
            avg_loss = total_loss / max(total_tokens, 1)
            pbar.set_postfix({
                'loss': f'{avg_loss:.4f}',
                'lr': f'{self.scheduler.get_last_lr()[0]:.6f}'
            })
            
            # Log to tensorboard
            if self.log_tensorboard and batch_idx % 1000 == 0:
                self.writer.add_scalar('train_loss_step', loss.item(), self.global_step)
                self.writer.add_scalar('learning_rate', self.scheduler.get_last_lr()[0], self.global_step)
                
                # Log gradient statistics
                self.log_gradients(self.global_step)
                self.log_weight_stats(self.global_step)
                
                # Log image predictions visualization (every 1000 steps)
                try:
                    texts = batch.get('texts', None)
                    # Use original outputs (before slicing) for visualization
                    pred_images = self.visualize_predictions(
                        images, input_ids, self.model(images, input_ids,attention_mask), 
                        texts=texts, num_samples=2
                    )
                    for i, pred_img in enumerate(pred_images):
                        self.writer.add_image(
                            f'train_predictions_sample_{i}',
                            np.array(pred_img),
                            self.global_step,
                            dataformats='HWC'
                        )
                except Exception as e:
                    print(f"Warning: Could not log prediction images: {e}")
                
                # # Generate and save prediction videos
                # if self.global_step % 10 == 0:
                try:
                    # Generate token prediction video with GT overlay
                    video_paths = self.generate_token_prediction_video(
                        images, input_ids, self.model(images, input_ids, attention_mask),
                        step=self.global_step, fps=2, num_samples=1
                    )
                    
                    if video_paths:
                        print(f"✓ Saved {len(video_paths)} token prediction video(s)")
                except Exception as e:
                    print(f"Warning: Could not generate videos: {e}")
            
                # self.global_step += 1
        
        return total_loss / max(total_tokens, 1)
    
    def validate(self, epoch=0):
        if self.val_loader is None:
            return None

        self.model.eval()
        total_loss = 0
        total_tokens = 0

        pbar = tqdm(self.val_loader, desc="Validation")
        for batch_idx, batch in enumerate(pbar):
            images = batch['images'].to(self.device)
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)

            # Forward pass
            with torch.no_grad():
                outputs = self.model(images, input_ids, attention_mask=attention_mask)

            # Prepare targets (same as training)
            num_img_tokens = 196
            B = input_ids.shape[0]
            
            # Create targets with proper shifting
            image_targets = torch.full((B, num_img_tokens), -100, device=self.device)
            text_targets = labels  # Already shifted labels from dataloader
            
            # Concatenate: [image_targets (ignore) | text_targets (actual predictions)]
            targets = torch.cat([image_targets, text_targets], dim=1)
            
            # Slice outputs to match target length
            outputs_for_loss = outputs[:, :targets.shape[1], :]
            
            # Validate shapes
            assert outputs_for_loss.shape[0] == targets.shape[0], f"Batch size mismatch: {outputs_for_loss.shape[0]} vs {targets.shape[0]}"
            assert outputs_for_loss.shape[1] == targets.shape[1], f"Sequence length mismatch: {outputs_for_loss.shape[1]} vs {targets.shape[1]}"

            # Calculate loss
            loss = self.criterion(
                outputs_for_loss.reshape(-1, outputs_for_loss.size(-1)),
                targets.reshape(-1)
            )

            # Calculate token-level loss
            batch_tokens = (targets != -100).sum().item()
            batch_loss = loss.item() * batch_tokens
            
            total_loss += batch_loss
            total_tokens += batch_tokens

            # Update progress bar with average loss
            avg_loss = total_loss / max(total_tokens, 1)
            pbar.set_postfix({'val_loss': f'{avg_loss:.4f}'})

            # Log validation predictions on first batch
            if self.log_tensorboard and batch_idx == 0:
                try:
                    texts = batch.get('texts', None)
                    pred_images = self.visualize_predictions(
                        images, input_ids, outputs, 
                        texts=texts, num_samples=2
                    )
                    for i, pred_img in enumerate(pred_images):
                        self.writer.add_image(
                            f'val_predictions_sample_{i}',
                            np.array(pred_img),
                            epoch,
                            dataformats='HWC'
                        )
                    
                    # Generate validation video
                    video_paths = self.generate_token_prediction_video(
                        images, input_ids, outputs,
                        step=epoch, fps=2, num_samples=1, num_img_tokens=196
                    )
                    if video_paths:
                        print(f"✓ Saved {len(video_paths)} validation video(s)")
                        
                except Exception as e:
                    print(f"Warning: Could not log validation prediction images: {e}")

        avg_loss = total_loss / max(total_tokens, 1)
        
        # Log validation loss to tensorboard
        if self.log_tensorboard:
            self.writer.add_scalar('val_loss_epoch', avg_loss, epoch)
        
        return avg_loss

    
    def train(self):
        print(f"Starting training for {self.max_epochs} epochs")
        print(f"Training samples: {len(self.train_loader.dataset)}")
        if self.val_loader:
            print(f"Validation samples: {len(self.val_loader.dataset)}")
        
        for epoch in range(self.max_epochs):
            # Train
            train_loss = self.train_epoch(epoch)
            
            # Validate
            val_loss = self.validate(epoch=epoch)
            
            # Log epoch results
            print(f"\nEpoch {epoch+1}/{self.max_epochs}")
            print(f"Train Loss: {train_loss:.4f}")
            if val_loss is not None:
                print(f"Val Loss: {val_loss:.4f}")
                
                # Save best model
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    self.save_checkpoint(f'best_model_epoch_{epoch+1}.pt')
                    print(f"Saved best model with val loss: {val_loss:.4f}")
            
            # Save checkpoint
            self.save_checkpoint(f'checkpoint_epoch_{epoch+1}.pt')
            
            # Log to tensorboard
            if self.log_tensorboard:
                self.writer.add_scalar('epoch_train_loss', train_loss, epoch+1)
                if val_loss is not None:
                    self.writer.add_scalar('epoch_val_loss', val_loss, epoch+1)
        
        print("Training completed!")
        if self.log_tensorboard and self.writer is not None:
            self.writer.close()
    
    def save_checkpoint(self, path):
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'epoch': self.max_epochs,
            'best_val_loss': self.best_val_loss,
            'model_config': {
                'vocab_size': self.model.token_embeds.num_embeddings,
                'embed_dim': self.model.token_embeds.embedding_dim
            }
        }
        torch.save(checkpoint, path)
        print(f"Checkpoint saved to {path}")
    
    def load_checkpoint(self, path):
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.best_val_loss = checkpoint['best_val_loss']
        print(f"Checkpoint loaded from {path}")


if __name__ == "__main__":
    
    # Use the same tokenizer as in your data loader
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
    vocab_size = tokenizer.vocab_size
    
    model = HaloVLM(
        vocab_size=vocab_size,
        emb_dim=512
    )
    
    # Get dataloaders (from your existing code)
    from lavis.datasets.builders import load_dataset
    import os
    
    os.environ['cache_root'] = "/home/ha/.cache/lavis/coco"
    
    coco_dataset = load_dataset("coco_caption")
    
    from dataloader import create_vlm_dataloaders
    
    print("\nCreating dataloaders...")
    dataloaders = create_vlm_dataloaders(
        coco_dataset,
        batch_size=8,
        num_workers=4,
        tokenizer=tokenizer,
        max_length=17
    )
    
    train_loader = dataloaders['train']
    val_loader = dataloaders.get('val', None)
    
    # Create trainer
    trainer = BasicVLMTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        learning_rate=3e-4,
        max_epochs=500,
        log_tensorboard=True,  # Set to True if you want tensorboard logging
        log_dir="./runs/vlm_training"
    )
    
    # Set tokenizer for prediction visualization
    trainer.set_tokenizer(tokenizer)
    
    # Start training
    trainer.train()
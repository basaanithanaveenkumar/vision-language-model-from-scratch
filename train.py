import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter
from models.vlm import BasicVLM
import torchvision.transforms as transforms
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import io
import cv2
import tempfile
import os
from datetime import datetime
from models.ha_vlm import HaloVLM


# majority of the code is generated by the copilot and deepseek for better logging features into the tensorboard

class BasicVLMTrainer:
    def __init__(self, model, train_loader, val_loader=None, 
                 device='cuda', learning_rate=1e-4, max_epochs=10,
                 log_tensorboard=False, log_dir="./runs"):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.max_epochs = max_epochs
        
        # Setup optimizer
        self.optimizer = optim.AdamW(
            self.model.parameters(), 
            lr=learning_rate,
            weight_decay=0.01
        )
        
        # Setup scheduler
        self.scheduler = CosineAnnealingLR(
            self.optimizer, 
            T_max=max_epochs * len(train_loader)
        )
        
        # Loss function - ignore padding tokens
        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)
        
        # Setup logging
        self.log_tensorboard = log_tensorboard
        self.writer = None
        if log_tensorboard:
            self.writer = SummaryWriter(log_dir=log_dir)
        
        self.best_val_loss = float('inf')
        self.global_step = 0
        self.tokenizer = None  # Will be set externally if needed
        
    def set_tokenizer(self, tokenizer):
        """Set tokenizer for decoding predictions"""
        self.tokenizer = tokenizer
    
    def log_gradients(self, step):
        """Log gradient statistics to tensorboard"""
        if not self.log_tensorboard or self.writer is None:
            return
        
        try:
            for name, param in self.model.named_parameters():
                if param.grad is not None:
                    # Log gradient norm
                    grad_norm = param.grad.data.norm(2)
                    self.writer.add_scalar(f'gradients/{name}/norm', grad_norm, step)
                    
                    # Log gradient statistics
                    self.writer.add_scalar(f'gradients/{name}/mean', param.grad.data.mean(), step)
                    self.writer.add_scalar(f'gradients/{name}/std', param.grad.data.std(), step)
                    self.writer.add_scalar(f'gradients/{name}/max', param.grad.data.max(), step)
                    self.writer.add_scalar(f'gradients/{name}/min', param.grad.data.min(), step)
                    
                    # Log gradient histogram
                    self.writer.add_histogram(f'gradients/{name}', param.grad.data, step)
        except Exception as e:
            print(f"Warning: Could not log gradients: {e}")
    
    def log_weight_stats(self, step):
        """Log weight statistics to tensorboard"""
        if not self.log_tensorboard or self.writer is None:
            return
        
        try:
            for name, param in self.model.named_parameters():
                if param.requires_grad:
                    # Log weight norm
                    weight_norm = param.data.norm(2)
                    self.writer.add_scalar(f'weights/{name}/norm', weight_norm, step)
                    
                    # Log weight statistics
                    self.writer.add_scalar(f'weights/{name}/mean', param.data.mean(), step)
                    self.writer.add_scalar(f'weights/{name}/std', param.data.std(), step)
                    
                    # Log weight histogram
                    self.writer.add_histogram(f'weights/{name}', param.data, step)
        except Exception as e:
            print(f"Warning: Could not log weight stats: {e}")
    
    def check_gradients(self):
        """Check for None gradients and print parameters with missing gradients"""
        print("\n" + "="*80)
        print("GRADIENT CHECK REPORT")
        print("="*80)
        
        none_gradient_params = []
        valid_gradient_params = []
        total_params_with_grad = 0
        total_params_without_grad = 0
        none_gradient_param_count = 0
        
        for name, param in self.model.named_parameters():
            total_params = param.numel()
            
            if param.grad is None:
                none_gradient_params.append({
                    'name': name,
                    'shape': tuple(param.shape),
                    'num_params': total_params,
                    'requires_grad': param.requires_grad,
                    'dtype': str(param.dtype)
                })
                if param.requires_grad:
                    none_gradient_param_count += total_params
                total_params_without_grad += total_params
            else:
                valid_gradient_params.append({
                    'name': name,
                    'shape': tuple(param.shape),
                    'num_params': total_params,
                    'grad_norm': param.grad.data.norm(2).item(),
                    'dtype': str(param.dtype)
                })
                total_params_with_grad += total_params
        
        # Print summary
        print(f"\n✓ Parameters WITH gradients: {len(valid_gradient_params)}")
        print(f"  Total parameter count: {total_params_with_grad:,}")
        
        print(f"\n✗ Parameters WITH NONE gradients: {len(none_gradient_params)}")
        print(f"  Total parameter count: {total_params_without_grad:,}")
        print(f"  Parameters with requires_grad=True but grad=None: {none_gradient_param_count:,}")
        
        # Print details of None gradient parameters
        if none_gradient_params:
            print(f"\n{'─'*80}")
            print("DETAILS OF PARAMETERS WITH NONE GRADIENTS:")
            print(f"{'─'*80}")
            print(f"{'Layer Name':<50} {'Shape':<15} {'Num Params':<12} {'Requires Grad':<15}")
            print(f"{'-'*80}")
            
            for param_info in none_gradient_params:
                requires_grad = "✓ Yes" if param_info['requires_grad'] else "✗ No"
                shape_str = str(param_info['shape'])
                print(f"{param_info['name']:<50} {shape_str:<15} {param_info['num_params']:<12,} {requires_grad:<15}")
        
        # Print details of valid gradient parameters (first 10)
        if valid_gradient_params:
            print(f"\n{'─'*80}")
            print("DETAILS OF PARAMETERS WITH VALID GRADIENTS (first 10):")
            print(f"{'─'*80}")
            print(f"{'Layer Name':<50} {'Shape':<15} {'Grad Norm':<15}")
            print(f"{'-'*80}")
            
            for i, param_info in enumerate(valid_gradient_params[:10]):
                shape_str = str(param_info['shape'])
                print(f"{param_info['name']:<50} {shape_str:<15} {param_info['grad_norm']:<15.6e}")
            
            if len(valid_gradient_params) > 10:
                print(f"... and {len(valid_gradient_params) - 10} more parameters with valid gradients")
        
        # Print overall statistics
        print(f"\n{'─'*80}")
        print("OVERALL STATISTICS:")
        print(f"{'─'*80}")
        total_model_params = sum(p.numel() for p in self.model.parameters())
        print(f"Total model parameters: {total_model_params:,}")
        print(f"Parameters with gradients: {total_params_with_grad:,} ({100*total_params_with_grad/total_model_params:.2f}%)")
        print(f"Parameters without gradients: {total_params_without_grad:,} ({100*total_params_without_grad/total_model_params:.2f}%)")
        
        if none_gradient_param_count > 0:
            print(f"\n⚠️  WARNING: {none_gradient_param_count:,} trainable parameters have None gradients!")
            print(f"   This may indicate frozen layers or computational graph issues.")
        else:
            print(f"\n✓ All trainable parameters have valid gradients!")
        
        print("="*80 + "\n")
        
        return {
            'total_params': total_model_params,
            'params_with_grad': total_params_with_grad,
            'params_without_grad': total_params_without_grad,
            'trainable_without_grad': none_gradient_param_count,
            'none_gradient_params': none_gradient_params,
            'valid_gradient_params': valid_gradient_params
        }
    
    def tensor_to_image(self, img_tensor):
        """Convert normalized image tensor to PIL Image"""
        # Handle batch dimension
        if img_tensor.dim() == 4:
            img_tensor = img_tensor[0]
        
        # Denormalize (ImageNet normalization)
        denormalize = transforms.Compose([
            transforms.Normalize(
                mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],
                std=[1/0.229, 1/0.224, 1/0.225]
            )
        ])
        
        img_tensor = denormalize(img_tensor.cpu())
        img_tensor = torch.clamp(img_tensor, 0, 1)
        img_array = img_tensor.permute(1, 2, 0).numpy()
        img_array = (img_array * 255).astype(np.uint8)
        return Image.fromarray(img_array)
    def visualize_predictions(self, images, input_ids, outputs, texts=None, num_samples=2, num_img_tokens=196):
        """
        Visualize predicted vs ground truth tokens with attractive formatting.
        Shows ground truth (GT) in green and predictions in blue with color-coded accuracy.
        
        Args:
            num_img_tokens: Number of image tokens to skip (default 196 for patch-based encoders)
        """
        num_samples = min(num_samples, images.size(0))
        visualizations = []
        
        for idx in range(num_samples):
            img = self.tensor_to_image(images[idx:idx+1])
            img_width, img_height = img.size
            
            # Get model predictions for this sample
            # Skip image tokens - only get text predictions
            logits = outputs[idx, num_img_tokens:, :]  # [text_seq_len, vocab_size]
            predictions = torch.argmax(logits, dim=-1)  # [text_seq_len]
            confidences = torch.softmax(logits, dim=-1).max(dim=-1)[0]  # [text_seq_len]
            
            # Ground truth is the labels (next tokens)
            # Since labels align with text positions, we use input_ids shifted by 1
            ground_truth = input_ids[idx, 1:]  # [seq_len-1] - next tokens
            
            # Trim predictions to match ground truth length
            predictions = predictions[:len(ground_truth)]
            confidences = confidences[:len(ground_truth)]
            
            # Get top-k predictions for alternative options
            top_k = 3
            top_probs, top_indices = torch.topk(torch.softmax(logits[:len(ground_truth)], dim=-1), top_k, dim=-1)
            
            # Create a larger canvas for better visualization
            canvas_height = img_height + 600
            
            # Try to use a better font if available
            try:
                title_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 16)
                text_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 13)
                small_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 11)
            except:
                title_font = ImageFont.load_default()
                text_font = ImageFont.load_default()
                small_font = ImageFont.load_default()
            
            # Calculate text content to determine canvas width
            gt_line = "GT: "
            for token_id in ground_truth[:min(20, len(ground_truth))]:
                token_id_item = token_id.item()
                if self.tokenizer:
                    try:
                        token_text = self.tokenizer.decode([token_id_item])
                        token_text = token_text.replace('[CLS]', '').replace('[SEP]', '').replace('[PAD]', '').strip()
                        if token_text:
                            gt_line += f"[{token_text}] "
                    except:
                        gt_line += f"[ID:{token_id_item}] "
                else:
                    gt_line += f"[{token_id_item}] "
            
            pred_line = "PRED: "
            for i, token_id in enumerate(predictions[:min(20, len(ground_truth))]):
                token_id_item = token_id.item()
                if self.tokenizer:
                    try:
                        token_text = self.tokenizer.decode([token_id_item])
                        token_text = token_text.replace('[CLS]', '').replace('[SEP]', '').replace('[PAD]', '').strip()
                        if token_text:
                            pred_line += f"[{token_text}] "
                    except:
                        pred_line += f"[ID:{token_id_item}] "
                else:
                    pred_line += f"[{token_id_item}] "
            
            # Use a temporary image to measure text
            temp_img = Image.new('RGB', (1, 1))
            temp_draw = ImageDraw.Draw(temp_img)
            
            try:
                gt_width = temp_draw.textbbox((0, 0), gt_line, font=text_font)[2] - temp_draw.textbbox((0, 0), gt_line, font=text_font)[0]
                pred_width = temp_draw.textbbox((0, 0), pred_line, font=text_font)[2] - temp_draw.textbbox((0, 0), pred_line, font=text_font)[0]
            except:
                gt_width = len(gt_line) * 8
                pred_width = len(pred_line) * 8
            
            # Set canvas width
            max_text_width = max(gt_width, pred_width)
            min_canvas_width = img_width
            required_width = max_text_width + 60
            canvas_width = max(min_canvas_width, required_width)
            
            canvas = Image.new('RGB', (canvas_width, canvas_height), color=(15, 15, 15))
            canvas.paste(img, (0, 0))
            
            draw = ImageDraw.Draw(canvas)
            
            y_offset = img_height + 15
            
            # Title section
            draw.text((15, y_offset), f"NEXT TOKEN PREDICTION (Skip first {num_img_tokens} image tokens)", 
                    fill=(100, 200, 255), font=title_font)
            y_offset += 35
            
            # Ground truth sequence
            draw.text((15, y_offset), "Ground Truth Sequence:", fill=(100, 255, 100), font=title_font)
            y_offset += 28
            draw.text((15, y_offset), gt_line, fill=(100, 255, 100), font=text_font)
            y_offset += 28
            
            # Predicted sequence
            draw.text((15, y_offset), "Predicted Sequence:", fill=(100, 150, 255), font=title_font)
            y_offset += 28
            draw.text((15, y_offset), pred_line, fill=(100, 150, 255), font=text_font)
            y_offset += 35
            
            # Detailed position analysis
            draw.text((15, y_offset), "Position-wise Analysis (First 10 text tokens):", 
                    fill=(200, 200, 200), font=title_font)
            y_offset += 28
            
            # Create position analysis table
            for pos in range(min(10, len(predictions))):
                pred_token = predictions[pos].item()
                confidence = confidences[pos].item()
                
                # Determine if prediction is correct
                is_correct = False
                if pos < len(ground_truth):
                    gt_token = ground_truth[pos].item()
                    is_correct = (pred_token == gt_token)
                
                # Color based on correctness and confidence
                if is_correct:
                    status_color = (100, 255, 100)  # Green
                    status_text = "✓"
                elif confidence > 0.6:
                    status_color = (255, 255, 100)  # Yellow
                    status_text = "≈"
                else:
                    status_color = (255, 100, 100)  # Red
                    status_text = "✗"
                
                # Build position line (show actual position in sequence after image tokens)
                actual_pos = num_img_tokens + pos
                pos_line = f"[{actual_pos:3d}→{pos:2d}] {status_text} "
                
                # Ground truth
                if pos < len(ground_truth):
                    gt_token = ground_truth[pos].item()
                    if self.tokenizer:
                        try:
                            gt_text = self.tokenizer.decode([gt_token]).strip()
                            pos_line += f"GT: '{gt_text}' | "
                        except:
                            pos_line += f"GT: ID{gt_token} | "
                    else:
                        pos_line += f"GT: ID{gt_token} | "
                
                # Prediction with confidence
                if self.tokenizer:
                    try:
                        pred_text = self.tokenizer.decode([pred_token]).strip()
                        pos_line += f"PRED: '{pred_text}' ({confidence:.2%})"
                    except:
                        pos_line += f"PRED: ID{pred_token} ({confidence:.2%})"
                else:
                    pos_line += f"PRED: ID{pred_token} ({confidence:.2%})"
                
                draw.text((15, y_offset), pos_line, fill=status_color, font=small_font)
                y_offset += 22
            
            # Add legend at the bottom
            y_offset += 10
            draw.text((15, y_offset), "Legend:", fill=(200, 200, 200), font=title_font)
            y_offset += 22
            draw.text((15, y_offset), "✓ Correct", fill=(100, 255, 100), font=small_font)
            draw.text((250, y_offset), "≈ Confident (~60%+)", fill=(255, 255, 100), font=small_font)
            draw.text((550, y_offset), "✗ Low confidence", fill=(255, 100, 100), font=small_font)
            
            # Calculate and display accuracy
            y_offset += 30
            accuracy = (predictions[:len(ground_truth)] == ground_truth).float().mean().item()
            accuracy_color = (100, 255, 100) if accuracy > 0.7 else (255, 255, 100) if accuracy > 0.4 else (255, 100, 100)
            draw.text((15, y_offset), f"Accuracy (Top-1): {accuracy:.2%} | Valid tokens: {len(ground_truth)}", 
                    fill=accuracy_color, font=title_font)
            
            visualizations.append(canvas)
        
        return visualizations

    def generate_token_prediction_video(self, images, input_ids, outputs, step=0, fps=2, num_samples=1, num_img_tokens=196, num_question_tokens=4):
        """
        Generate visually enhanced video showing token-by-token predictions with circular tokens
        num_img_tokens: Number of vision tokens prepended before text tokens
        num_question_tokens: Number of question tokens before answer starts
        """
        try:
            import imageio
            import cv2
        except ImportError:
            print("Warning: imageio or cv2 not installed. Install with: pip install imageio opencv-python")
            return []
        
        os.makedirs('./videos', exist_ok=True)
        num_samples = min(num_samples, images.size(0))
        saved_paths = []
        
        # Enhanced color palette
        COLORS = {
            'bg': (25, 28, 35),           # Dark background
            'panel_bg': (40, 44, 52),     # Panel background
            'correct': (80, 250, 123),    # Green for correct
            'incorrect': (255, 85, 85),   # Red for incorrect
            'gt_text': (139, 233, 253),   # Cyan for GT
            'pred_text': (189, 147, 249), # Purple for predictions
            'question': (255, 193, 7),    # Amber for questions
            'accent': (255, 184, 108),    # Orange accent
            'progress': (98, 114, 164),   # Blue-gray for progress bar
            'text': (248, 248, 242),      # Off-white text
            'dim': (155, 155, 155),       # Dimmed text
            'vision': (255, 121, 198)     # Pink for vision tokens
        }
        
        for sample_idx in range(num_samples):
            try:
                idx = sample_idx
                img = self.tensor_to_image(images[idx:idx+1])
                img_width, img_height = img.size
                
                logits = outputs[idx]
                predictions = torch.argmax(logits, dim=-1)
                confidences = torch.softmax(logits, dim=-1).max(dim=-1)[0]
                
                # Skip vision tokens - text tokens start after num_img_tokens
                text_predictions = predictions[num_img_tokens:]
                text_confidences = confidences[num_img_tokens:]
                
                # Separate question and answer tokens
                question_predictions = text_predictions[:num_question_tokens]
                answer_predictions = text_predictions[num_question_tokens:]
                
                question_confidences = text_confidences[:num_question_tokens]
                answer_confidences = text_confidences[num_question_tokens:]
                
                # Ground truth alignment
                ground_truth = input_ids[idx, 1:]  # Remove BOS/first token
                question_gt = ground_truth[:num_question_tokens]
                answer_gt = ground_truth[num_question_tokens:]
                
                # Load fonts with multiple sizes
                try:
                    title_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 18)
                    text_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 14)
                    small_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 12)
                    mono_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSansMono.ttf", 13)
                except:
                    title_font = text_font = small_font = mono_font = ImageFont.load_default()
                
                # Enhanced video dimensions
                panel_height = 350  # Increased for question section
                video_width = max(img_width, 800)
                video_height = img_height + panel_height
                
                frames = []
                max_tokens = min(20, len(text_predictions))
                
                # Pre-compute ground truth tokens
                gt_tokens = []
                for token_id in ground_truth[:max_tokens]:
                    token_id_item = token_id.item()
                    if self.tokenizer:
                        try:
                            decoded = self.tokenizer.decode([token_id_item]).strip()
                            if decoded and decoded not in ['[CLS]', '[SEP]', '[PAD]']:
                                gt_tokens.append(decoded)
                        except:
                            gt_tokens.append(f"[{token_id_item}]")
                    else:
                        gt_tokens.append(f"[{token_id_item}]")
                
                # Generate frames
                for pos in range(max_tokens):
                    # Create frame
                    frame_img = Image.new('RGB', (video_width, video_height), color=COLORS['bg'])
                    
                    # Resize and center image
                    if img_width < video_width:
                        offset_x = (video_width - img_width) // 2
                        frame_img.paste(img, (offset_x, 0))
                    else:
                        frame_img.paste(img, (0, 0))
                    
                    draw = ImageDraw.Draw(frame_img)
                    
                    # Draw panel background
                    panel_y = img_height
                    draw.rectangle([0, panel_y, video_width, video_height], fill=COLORS['panel_bg'])
                    
                    # Draw accent line separator
                    draw.rectangle([0, panel_y, video_width, panel_y + 3], fill=COLORS['accent'])
                    
                    y_offset = panel_y + 15
                    x_margin = 20
                    
                    # Vision tokens indicator
                    vision_info = f"Vision Tokens: {num_img_tokens} (processed)"
                    draw.text((x_margin, y_offset), vision_info, fill=COLORS['vision'], font=small_font)
                    y_offset += 20
                    
                    # Progress bar
                    progress = (pos + 1) / max_tokens
                    bar_width = video_width - 2 * x_margin
                    bar_height = 8
                    # Background bar
                    draw.rectangle([x_margin, y_offset, x_margin + bar_width, y_offset + bar_height], 
                                fill=COLORS['progress'], outline=COLORS['dim'])
                    # Progress fill
                    fill_width = int(bar_width * progress)
                    draw.rectangle([x_margin, y_offset, x_margin + fill_width, y_offset + bar_height], 
                                fill=COLORS['accent'])
                    # Progress text
                    progress_text = f"{pos + 1}/{max_tokens} tokens"
                    draw.text((video_width - x_margin - 130, y_offset - 2), progress_text, 
                            fill=COLORS['dim'], font=small_font)
                    
                    y_offset += 25
                    
                    # Question Section (only show if we're past question tokens)
                    if pos >= num_question_tokens:
                        draw.text((x_margin, y_offset), "QUESTION", fill=COLORS['question'], font=title_font)
                        y_offset += 28
                        
                        # Draw question tokens (all at once)
                        x_pos = x_margin
                        ellipse_height = 28
                        for i in range(num_question_tokens):
                            if i >= len(question_predictions):
                                break
                            
                            pred_token = question_predictions[i].item()
                            if self.tokenizer:
                                try:
                                    decoded = self.tokenizer.decode([pred_token]).strip()
                                    if not decoded or decoded in ['[CLS]', '[SEP]', '[PAD]']:
                                        decoded = f"[{pred_token}]"
                                except:
                                    decoded = f"[{pred_token}]"
                            else:
                                decoded = f"[{pred_token}]"
                            
                            text_bbox = draw.textbbox((0, 0), decoded, font=small_font)
                            text_width = text_bbox[2] - text_bbox[0]
                            ellipse_width = max(text_width + 16, ellipse_height)
                            
                            if x_pos + ellipse_width > video_width - x_margin:
                                break
                            
                            center_x = x_pos + ellipse_width // 2
                            center_y = y_offset + ellipse_height // 2
                            
                            # Draw ellipse with question color
                            draw.ellipse([x_pos, y_offset, x_pos + ellipse_width, y_offset + ellipse_height],
                                        fill=(50, 55, 65), outline=COLORS['question'], width=2)
                            
                            # Center text
                            text_x = center_x - text_width // 2
                            text_y = center_y - 6
                            draw.text((text_x, text_y), decoded, fill=COLORS['question'], font=small_font)
                            
                            x_pos += ellipse_width + 6
                        
                        y_offset += 38
                    
                    # Ground Truth Section (for answer part)
                    if pos < num_question_tokens:
                        section_label = "GROUND TRUTH (Question + Answer)"
                    else:
                        section_label = "GROUND TRUTH (Answer)"
                        
                    draw.text((x_margin, y_offset), section_label, fill=COLORS['gt_text'], font=title_font)
                    y_offset += 28
                    
                    # Draw GT tokens
                    x_pos = x_margin
                    ellipse_height = 32
                    display_end = pos + 1 if pos < num_question_tokens else (pos + 1)
                    
                    for i, token in enumerate(gt_tokens[:display_end]):
                        # Calculate ellipse width
                        text_bbox = draw.textbbox((0, 0), token, font=mono_font)
                        text_width = text_bbox[2] - text_bbox[0]
                        ellipse_width = max(text_width + 20, ellipse_height)
                        
                        if x_pos + ellipse_width > video_width - x_margin:
                            break
                        
                        center_x = x_pos + ellipse_width // 2
                        center_y = y_offset + ellipse_height // 2
                        
                        # Use different color for question tokens
                        outline_color = COLORS['question'] if i < num_question_tokens else COLORS['gt_text']
                        text_color = COLORS['question'] if i < num_question_tokens else COLORS['gt_text']
                        
                        # Draw ellipse
                        draw.ellipse([x_pos, y_offset, x_pos + ellipse_width, y_offset + ellipse_height],
                                    fill=(50, 55, 65), outline=outline_color, width=2)
                        
                        # Center text
                        text_x = center_x - text_width // 2
                        text_y = center_y - 7
                        draw.text((text_x, text_y), token, fill=text_color, font=mono_font)
                        
                        x_pos += ellipse_width + 8
                    
                    y_offset += 42
                    
                    # Prediction Section
                    draw.text((x_margin, y_offset), "PREDICTIONS", fill=COLORS['pred_text'], font=title_font)
                    
                    # Calculate accuracy (only for answer part if past questions)
                    correct_count = 0
                    total_count = pos + 1
                    
                    if pos >= num_question_tokens:
                        # Only count answer accuracy
                        answer_pos = pos - num_question_tokens
                        for i in range(answer_pos + 1):
                            if i < len(answer_gt) and answer_predictions[i].item() == answer_gt[i].item():
                                correct_count += 1
                        total_count = answer_pos + 1
                        acc_label = "Answer Acc:"
                    else:
                        # Include all tokens
                        for i in range(pos + 1):
                            if i < len(ground_truth) and text_predictions[i].item() == ground_truth[i].item():
                                correct_count += 1
                        acc_label = "Acc:"
                    
                    accuracy = (correct_count / max(total_count, 1)) * 100
                    
                    # Draw accuracy badge
                    acc_text = f"{acc_label} {accuracy:.0f}%"
                    acc_color = COLORS['correct'] if accuracy >= 70 else (COLORS['accent'] if accuracy >= 50 else COLORS['incorrect'])
                    draw.text((video_width - x_margin - 150, y_offset), acc_text, 
                            fill=acc_color, font=title_font)
                    
                    y_offset += 28
                    
                    # Draw prediction tokens
                    x_pos = x_margin
                    for i in range(pos + 1):
                        pred_token = text_predictions[i].item()
                        confidence = text_confidences[i].item()
                        
                        if self.tokenizer:
                            try:
                                decoded = self.tokenizer.decode([pred_token]).strip()
                                if not decoded or decoded in ['[CLS]', '[SEP]', '[PAD]']:
                                    decoded = f"[{pred_token}]"
                            except:
                                decoded = f"[{pred_token}]"
                        else:
                            decoded = f"[{pred_token}]"
                        
                        # Check correctness
                        is_correct = (i < len(ground_truth) and pred_token == ground_truth[i].item())
                        
                        # Use question color for first 4 tokens, then correct/incorrect
                        if i < num_question_tokens:
                            token_color = COLORS['question']
                        else:
                            token_color = COLORS['correct'] if is_correct else COLORS['incorrect']
                        
                        # Calculate ellipse dimensions
                        text_bbox = draw.textbbox((0, 0), decoded, font=mono_font)
                        text_width = text_bbox[2] - text_bbox[0]
                        ellipse_height = 32
                        ellipse_width = max(text_width + 20, ellipse_height)
                        
                        if x_pos + ellipse_width > video_width - x_margin:
                            break
                        
                        center_x = x_pos + ellipse_width // 2
                        center_y = y_offset + ellipse_height // 2
                        
                        # Glow effect for last token
                        if i == pos:
                            draw.ellipse([x_pos - 3, y_offset - 3, 
                                        x_pos + ellipse_width + 3, y_offset + ellipse_height + 3],
                                        fill=None, outline=token_color, width=3)
                        
                        # Draw main ellipse
                        draw.ellipse([x_pos, y_offset, x_pos + ellipse_width, y_offset + ellipse_height],
                                    fill=(50, 55, 65), outline=token_color, width=2)
                        
                        # Center text
                        text_x = center_x - text_width // 2
                        text_y = center_y - 7
                        draw.text((text_x, text_y), decoded, fill=token_color, font=mono_font)
                        
                        # Confidence indicator
                        if confidence > 0:
                            arc_y = y_offset + ellipse_height + 3
                            arc_width = int(ellipse_width * confidence)
                            arc_x = center_x - arc_width // 2
                            draw.rectangle([arc_x, arc_y, arc_x + arc_width, arc_y + 3],
                                        fill=token_color)
                        
                        x_pos += ellipse_width + 8
                    
                    # Convert to numpy array
                    frame_array = np.array(frame_img, dtype=np.uint8)
                    frames.append(frame_array)
                
                # Save video
                if frames:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    video_path = f"./videos/token_pred_step{step}_sample{sample_idx}_{timestamp}.mp4"
                    
                    imageio.mimwrite(video_path, frames, fps=fps, codec='libx264', 
                                    quality=8, pixelformat='yuv420p')
                    saved_paths.append(video_path)
                    print(f"✓ Saved enhanced token prediction video: {video_path}")
            
            except Exception as e:
                print(f"Warning: Could not generate video for sample {sample_idx}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        return saved_paths

    def train_epoch(self, epoch):
        self.model.train()
        total_loss = 0
        total_tokens = 0
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch+1}")
        for batch_idx, batch in enumerate(pbar):
        # single batch overfitting code
        # single_batch = next(iter(self.train_loader))

        # # Train on this single batch repeatedly
        # pbar = tqdm(range(len(self.train_loader)), desc=f"Epoch {epoch+1}")
        # for batch_idx in pbar:
        #     batch = single_batch
            # Move data to device
            images = batch['images'].to(self.device)
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)  # ✓ ADD THIS
            labels = batch['labels'].to(self.device)  # ✓ USE LABELS FROM DATALOADER
            
            # Validate that labels are present
            if labels is None:
                raise ValueError("Batch['labels'] is None! Dataloader must provide labels.")
            
            
            # Forward pass
            self.optimizer.zero_grad()
            
            # Get model outputs
            #outputs = self.model(images, input_ids)
            outputs = self.model(images, input_ids, attention_mask=attention_mask)
            
            # Prepare targets for loss calculation
            # For next-token prediction:
            # Input sequence to model: [IMG_TOKEN, text_0, text_1, ..., text_{n-1}]  (n+1 tokens total)
            # Model output at pos i:   predicts the token that comes AFTER position i
            # 
            # So: output[0] (at IMG_TOKEN) → should predict text_0
            #     output[1] (at text_0)    → should predict text_1
            #     output[i]                → should predict text_i
            #     output[n]                → should predict text_n (but we don't have this in input_ids)
            #
            # Therefore:
            # - image_targets at pos 0:     -100 (ignore, image token has no language target)
            # - text_targets at pos 1..n:   input_ids[1:n+1] (the next tokens)
            # - pos n+1:                    ignored (no target available)
            
            num_img_tokens = 196
            B = input_ids.shape[0]
            seq_len = input_ids.shape[1]  # Original text sequence length
            
            # Create targets with proper shifting
            image_targets = torch.full((B, num_img_tokens), -100, device=self.device)  # [B, 1]
            text_targets = labels  # [B, seq_len-1] - next tokens for text positions
            
            # Concatenate: [image_targets (ignore) | text_targets (actual predictions)]
            # Final shape: [B, 1 + (seq_len-1)] = [B, seq_len]
            targets = torch.cat([image_targets, text_targets], dim=1)
            # use labels directly without concatenation and shifting as we have already added image token in the beginnning
            #targets = text_targets
            # Model output shape: [B, 1 + seq_len, vocab_size]
            # We only compute loss for first seq_len positions (last position is discarded)
            # outputs_for_loss shape: [B, seq_len, vocab_size]
            # outputs_for_loss = outputs[:, :seq_len, :]
            outputs_for_loss = outputs[:, :targets.shape[1], :]
            assert outputs_for_loss.shape[0] == targets.shape[0], f"Batch size mismatch: {outputs_for_loss.shape[0]} vs {labels.shape[0]}"
            assert outputs_for_loss.shape[1] == targets.shape[1], f"Sequence length mismatch: {outputs_for_loss.shape[1]} vs {labels.shape[1]}"
            # for debugging
            #print(input_ids[0], "input ids")
            #print(targets[0], "tagrgets")
            
            # Calculate loss
            # This computes cross-entropy for:
            # - Position 0 (image): loss ignored due to -100 target
            # - Positions 1..seq_len-1: actual next-token prediction losses
            loss = self.criterion(
            outputs_for_loss.reshape(-1, outputs_for_loss.size(-1)),
            targets.reshape(-1)  # targets are already shifted and padded properly
            )
            
            # Print output IDs (predictions) after loss calculation
            output_ids = torch.argmax(outputs_for_loss, dim=-1)  # [B, seq_len]
            
            # for debugging
            #print(output_ids[0], "output ids")

            # Backward pass
            loss.backward()
            
            # Check gradients on first batch of first epoch
            if epoch == 0 and batch_idx == 0:
                print("\nChecking gradients after first backward pass...")
                self.check_gradients()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            
            # Optimizer step
            self.optimizer.step()
            self.scheduler.step()
            
            # Calculate token-level loss for logging
            batch_tokens = (targets != -100).sum().item()
            batch_loss = loss.item() * batch_tokens
            
            total_loss += batch_loss
            total_tokens += batch_tokens
            
            # Update progress bar
            avg_loss = total_loss / max(total_tokens, 1)
            pbar.set_postfix({
                'loss': f'{avg_loss:.4f}',
                'lr': f'{self.scheduler.get_last_lr()[0]:.6f}'
            })
            
            # Log to tensorboard
            if self.log_tensorboard and batch_idx % 1000 == 0:
                self.writer.add_scalar('train_loss_step', loss.item(), self.global_step)
                self.writer.add_scalar('learning_rate', self.scheduler.get_last_lr()[0], self.global_step)
                
                # Log gradient statistics
                self.log_gradients(self.global_step)
                self.log_weight_stats(self.global_step)
                
                # Log image predictions visualization (every 1000 steps)
                try:
                    texts = batch.get('texts', None)
                    # Use original outputs (before slicing) for visualization
                    pred_images = self.visualize_predictions(
                        images, input_ids, self.model(images, input_ids,attention_mask), 
                        texts=texts, num_samples=2
                    )
                    for i, pred_img in enumerate(pred_images):
                        self.writer.add_image(
                            f'train_predictions_sample_{i}',
                            np.array(pred_img),
                            self.global_step,
                            dataformats='HWC'
                        )
                except Exception as e:
                    print(f"Warning: Could not log prediction images: {e}")
                
                # # Generate and save prediction videos
                # if self.global_step % 10 == 0:
                try:
                    # Generate token prediction video with GT overlay
                    video_paths = self.generate_token_prediction_video(
                        images, input_ids, self.model(images, input_ids, attention_mask),
                        step=self.global_step, fps=2, num_samples=1
                    )
                    
                    if video_paths:
                        print(f"✓ Saved {len(video_paths)} token prediction video(s)")
                except Exception as e:
                    print(f"Warning: Could not generate videos: {e}")
            
                # self.global_step += 1
        
        return total_loss / max(total_tokens, 1)
    
    def validate(self, epoch=0):
        if self.val_loader is None:
            return None

        self.model.eval()
        total_loss = 0
        total_tokens = 0

        pbar = tqdm(self.val_loader, desc="Validation")
        for batch_idx, batch in enumerate(pbar):
            images = batch['images'].to(self.device)
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)

            # Forward pass
            with torch.no_grad():
                outputs = self.model(images, input_ids, attention_mask=attention_mask)

            # Prepare targets (same as training)
            num_img_tokens = 196
            B = input_ids.shape[0]
            
            # Create targets with proper shifting
            image_targets = torch.full((B, num_img_tokens), -100, device=self.device)
            text_targets = labels  # Already shifted labels from dataloader
            
            # Concatenate: [image_targets (ignore) | text_targets (actual predictions)]
            targets = torch.cat([image_targets, text_targets], dim=1)
            
            # Slice outputs to match target length
            outputs_for_loss = outputs[:, :targets.shape[1], :]
            
            # Validate shapes
            assert outputs_for_loss.shape[0] == targets.shape[0], f"Batch size mismatch: {outputs_for_loss.shape[0]} vs {targets.shape[0]}"
            assert outputs_for_loss.shape[1] == targets.shape[1], f"Sequence length mismatch: {outputs_for_loss.shape[1]} vs {targets.shape[1]}"

            # Calculate loss
            loss = self.criterion(
                outputs_for_loss.reshape(-1, outputs_for_loss.size(-1)),
                targets.reshape(-1)
            )

            # Calculate token-level loss
            batch_tokens = (targets != -100).sum().item()
            batch_loss = loss.item() * batch_tokens
            
            total_loss += batch_loss
            total_tokens += batch_tokens

            # Update progress bar with average loss
            avg_loss = total_loss / max(total_tokens, 1)
            pbar.set_postfix({'val_loss': f'{avg_loss:.4f}'})

            # Log validation predictions on first batch
            if self.log_tensorboard and batch_idx == 0:
                try:
                    texts = batch.get('texts', None)
                    pred_images = self.visualize_predictions(
                        images, input_ids, outputs, 
                        texts=texts, num_samples=2
                    )
                    for i, pred_img in enumerate(pred_images):
                        self.writer.add_image(
                            f'val_predictions_sample_{i}',
                            np.array(pred_img),
                            epoch,
                            dataformats='HWC'
                        )
                    
                    # Generate validation video
                    video_paths = self.generate_token_prediction_video(
                        images, input_ids, outputs,
                        step=epoch, fps=2, num_samples=1, num_img_tokens=196
                    )
                    if video_paths:
                        print(f"✓ Saved {len(video_paths)} validation video(s)")
                        
                except Exception as e:
                    print(f"Warning: Could not log validation prediction images: {e}")

        avg_loss = total_loss / max(total_tokens, 1)
        
        # Log validation loss to tensorboard
        if self.log_tensorboard:
            self.writer.add_scalar('val_loss_epoch', avg_loss, epoch)
        
        return avg_loss

    
    def train(self):
        print(f"Starting training for {self.max_epochs} epochs")
        print(f"Training samples: {len(self.train_loader.dataset)}")
        if self.val_loader:
            print(f"Validation samples: {len(self.val_loader.dataset)}")
        
        for epoch in range(self.max_epochs):
            # Train
            train_loss = self.train_epoch(epoch)
            
            # Validate
            val_loss = self.validate(epoch=epoch)
            
            # Log epoch results
            print(f"\nEpoch {epoch+1}/{self.max_epochs}")
            print(f"Train Loss: {train_loss:.4f}")
            if val_loss is not None:
                print(f"Val Loss: {val_loss:.4f}")
                
                # Save best model
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    self.save_checkpoint(f'best_model_epoch_{epoch+1}.pt')
                    print(f"Saved best model with val loss: {val_loss:.4f}")
            
            # Save checkpoint
            self.save_checkpoint(f'checkpoint_epoch_{epoch+1}.pt')
            
            # Log to tensorboard
            if self.log_tensorboard:
                self.writer.add_scalar('epoch_train_loss', train_loss, epoch+1)
                if val_loss is not None:
                    self.writer.add_scalar('epoch_val_loss', val_loss, epoch+1)
        
        print("Training completed!")
        if self.log_tensorboard and self.writer is not None:
            self.writer.close()
    
    def save_checkpoint(self, path):
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'epoch': self.max_epochs,
            'best_val_loss': self.best_val_loss,
            'model_config': {
                'vocab_size': self.model.token_embeds.num_embeddings,
                'embed_dim': self.model.token_embeds.embedding_dim
            }
        }
        torch.save(checkpoint, path)
        print(f"Checkpoint saved to {path}")
    
    def load_checkpoint(self, path):
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.best_val_loss = checkpoint['best_val_loss']
        print(f"Checkpoint loaded from {path}")


if __name__ == "__main__":
    
    # Use the same tokenizer as in your data loader
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
    vocab_size = tokenizer.vocab_size
    
    model = HaloVLM(
        vocab_size=vocab_size,
        emb_dim=512
    )
    
    # Get dataloaders (from your existing code)
    from lavis.datasets.builders import load_dataset
    import os
    
    os.environ['cache_root'] = "/home/ha/.cache/lavis/coco"
    
    coco_dataset = load_dataset("coco_caption")
    
    from dataloader import create_vlm_dataloaders
    
    print("\nCreating dataloaders...")
    dataloaders = create_vlm_dataloaders(
        coco_dataset,
        batch_size=24,
        num_workers=4,
        tokenizer=tokenizer,
        max_length=17
    )
    
    train_loader = dataloaders['train']
    val_loader = dataloaders.get('val', None)
    
    # Create trainer
    trainer = BasicVLMTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        device='cuda' if torch.cuda.is_available() else 'cpu',
        learning_rate=3e-4,
        max_epochs=500,
        log_tensorboard=True,  # Set to True if you want tensorboard logging
        log_dir="./runs/vlm_training"
    )
    
    # Set tokenizer for prediction visualization
    trainer.set_tokenizer(tokenizer)
    
    # Start training
    trainer.train()